/**
 * Anthropic API Client
 * 
 * Core API client for communicating with Claude via Anthropic's API.
 * Handles authentication, request formatting, streaming responses, and error handling.
 * 
 * Key chunks analyzed:
 * - chunk_0557.js:81-144 (y$ function - API client factory)
 * - chunk_0581.js:2-300+ (KSB function - streaming API calls)
 * - chunk_0621.js:418-559 (wR function - conversation streaming engine)
 * - chunk_0580.js:684-688 (V01 function - API wrapper)
 */

import { AnthropicBedrock } from '@anthropic-ai/bedrock-sdk';
import { AnthropicVertex } from '@anthropic-ai/vertex-sdk';
import Anthropic from '@anthropic-ai/sdk';
import { configManager } from '../config/manager.js';
import { telemetryManager } from '../telemetry/manager.js';
import { 
    ModelOverloadError, 
    APIError, 
    handleStreamError, 
    generateErrorMessage, 
    generateFallbackMessages,
    createSystemMessage 
} from '../utils/error-handler.js';
import { withAPIRetry, withTimeout } from '../utils/retry-handler.js';

/**
 * API Client Configuration
 * Extracted from chunk_0557.js:81-144 (y$ function)
 */
const DEFAULT_TIMEOUT_MS = 60000;
const DEFAULT_MAX_RETRIES = 0;

/**
 * Create Anthropic API Client
 * 
 * Extracted from chunk_0557.js:81-144 (y$ function):
 * - Multi-provider support (Anthropic, Bedrock, Vertex AI)
 * - Environment-based configuration
 * - Authentication handling (API keys, tokens, AWS credentials)
 * - Custom headers and timeout configuration
 * - Browser compatibility settings
 * 
 * @param {Object} options - Client configuration options
 * @returns {Promise<Object>} Configured API client
 */
export async function createAPIClient({\n    apiKey,\n    maxRetries = DEFAULT_MAX_RETRIES,\n    model,\n    isNonInteractiveSession = false,\n    isSmallFastModel = false\n} = {}) {\n    // Build default headers (from chunk_0557.js:88-92)\n    const defaultHeaders = {\n        'x-app': 'cli',\n        'User-Agent': getUserAgent(),\n        ...getCustomHeaders()\n    };\n    \n    // Add authentication headers if needed\n    await initializeAuthentication();\n    if (!isWebEnvironment()) {\n        addAuthenticationHeaders(defaultHeaders);\n    }\n    \n    // Base client configuration\n    const baseConfig = {\n        defaultHeaders,\n        maxRetries,\n        timeout: parseInt(process.env.API_TIMEOUT_MS || String(DEFAULT_TIMEOUT_MS), 10),\n        dangerouslyAllowBrowser: true,\n        fetchOptions: getFetchOptions(),\n        ...(shouldUseCustomFetch() && {\n            fetch: getCustomFetch()\n        })\n    };\n    \n    // AWS Bedrock configuration (from chunk_0557.js:105-123)\n    if (process.env.CLAUDE_CODE_USE_BEDROCK) {\n        const awsRegion = (isSmallFastModel && process.env.ANTHROPIC_SMALL_FAST_MODEL_AWS_REGION) \n            ? process.env.ANTHROPIC_SMALL_FAST_MODEL_AWS_REGION \n            : getDefaultAWSRegion();\n        \n        const bedrockConfig = {\n            ...baseConfig,\n            awsRegion,\n            ...(process.env.CLAUDE_CODE_SKIP_BEDROCK_AUTH && {\n                skipAuth: true\n            })\n        };\n        \n        // Handle AWS Bearer Token authentication\n        if (process.env.AWS_BEARER_TOKEN_BEDROCK) {\n            bedrockConfig.skipAuth = true;\n            bedrockConfig.defaultHeaders = {\n                ...bedrockConfig.defaultHeaders,\n                Authorization: `Bearer ${process.env.AWS_BEARER_TOKEN_BEDROCK}`\n            };\n        } else {\n            // Get AWS credentials\n            const awsCredentials = await getAWSCredentials();\n            if (awsCredentials) {\n                bedrockConfig.awsAccessKey = awsCredentials.accessKeyId;\n                bedrockConfig.awsSecretKey = awsCredentials.secretAccessKey;\n                bedrockConfig.awsSessionToken = awsCredentials.sessionToken;\n            }\n        }\n        \n        return new AnthropicBedrock(bedrockConfig);\n    }\n    \n    // Google Vertex AI configuration (from chunk_0557.js:124-137)\n    if (process.env.CLAUDE_CODE_USE_VERTEX) {\n        const vertexConfig = {\n            ...baseConfig,\n            region: getVertexRegion(model),\n            ...(process.env.CLAUDE_CODE_SKIP_VERTEX_AUTH && {\n                googleAuth: {\n                    getClient: () => ({\n                        getRequestHeaders: () => ({})\n                    })\n                }\n            })\n        };\n        \n        return new AnthropicVertex(vertexConfig);\n    }\n    \n    // Standard Anthropic API configuration (from chunk_0557.js:138-144)\n    const anthropicConfig = {\n        apiKey: isWebEnvironment() ? null : (apiKey || getAPIKey(isNonInteractiveSession)),\n        authToken: isWebEnvironment() ? getWebAuthToken()?.accessToken : undefined,\n        ...baseConfig\n    };\n    \n    return new Anthropic(anthropicConfig);\n}\n\n/**\n * Streaming Conversation Engine\n * \n * Extracted from chunk_0621.js:418-559 (wR function):\n * - Recursive conversation flow with tool execution\n * - Message compaction and context management\n * - Model fallback logic and error handling\n * - Streaming response processing\n * - Queue command execution integration\n * \n * @param {Object} params - Conversation parameters\n * @returns {AsyncGenerator} Streaming conversation chunks\n */\nexport async function* streamConversation({\n    messages,\n    systemPrompt,\n    userContext,\n    systemContext,\n    canUseTool,\n    toolUseContext,\n    autoCompactTracking,\n    fallbackModel,\n    stopHookActive,\n    promptCategory\n}) {\n    yield {\n        type: 'stream_request_start'\n    };\n    \n    let currentMessages = messages;\n    let compactionTracking = autoCompactTracking;\n    \n    // Message compaction (from chunk_0621.js:435-451)\n    const compactionResult = await compactMessages(currentMessages);\n    currentMessages = compactionResult.messages;\n    \n    if (compactionResult.compactionInfo?.systemMessage) {\n        yield compactionResult.compactionInfo.systemMessage;\n    }\n    \n    // Auto-compact messages if needed\n    const { messages: finalMessages, wasCompacted } = await autoCompactMessages(currentMessages, toolUseContext);\n    \n    if (wasCompacted) {\n        telemetryManager.track('tengu_auto_compact_succeeded', {\n            originalMessageCount: messages.length,\n            compactedMessageCount: finalMessages.length\n        });\n        \n        if (!compactionTracking?.compacted) {\n            compactionTracking = {\n                compacted: true,\n                turnId: generateTurnId(),\n                turnCounter: 0\n            };\n        }\n        currentMessages = finalMessages;\n    }\n    \n    let assistantMessages = [];\n    let currentModel = resolveModelForQuery({\n        permissionMode: toolUseContext.getToolPermissionContext().mode,\n        mainLoopModel: toolUseContext.options.mainLoopModel\n    });\n    let shouldRetry = true;\n    \n    try {\n        while (shouldRetry) {\n            shouldRetry = false;\n            \n            try {\n                let fallbackTriggered = false;\n                \n                // Stream API response (from chunk_0621.js:463-477)\n                for await (const chunk of streamAPIRequest(\n                    formatMessagesForAPI(currentMessages, userContext),\n                    buildSystemPrompt(systemPrompt, systemContext),\n                    toolUseContext.options.maxThinkingTokens,\n                    toolUseContext.options.tools,\n                    toolUseContext.abortController.signal,\n                    {\n                        getToolPermissionContext: toolUseContext.getToolPermissionContext,\n                        model: currentModel,\n                        prependCLISysprompt: true,\n                        toolChoice: undefined,\n                        isNonInteractiveSession: toolUseContext.options.isNonInteractiveSession,\n                        fallbackModel,\n                        onStreamingFallback: () => {\n                            fallbackTriggered = true;\n                        },\n                        promptCategory\n                    }\n                )) {\n                    if (fallbackTriggered) {\n                        yield* generateFallbackMessages(assistantMessages, 'Streaming fallback triggered');\n                        assistantMessages.length = 0;\n                    }\n                    \n                    if (yield chunk; chunk.type === 'assistant') {\n                        assistantMessages.push(chunk);\n                    }\n                }\n                \n            } catch (error) {\n                // Model fallback handling (from chunk_0621.js:479-488)\n                if (error instanceof ModelOverloadError && fallbackModel) {\n                    currentModel = fallbackModel;\n                    shouldRetry = true;\n                    yield* generateFallbackMessages(assistantMessages, 'Model fallback triggered');\n                    assistantMessages.length = 0;\n                    toolUseContext.options.mainLoopModel = fallbackModel;\n                    \n                    telemetryManager.track('tengu_model_fallback_triggered', {\n                        original_model: error.originalModel,\n                        fallback_model: fallbackModel,\n                        entrypoint: 'cli'\n                    });\n                    \n                    yield createSystemMessage(`Model fallback triggered: switching from ${error.originalModel} to ${error.fallbackModel}`, 'info');\n                    continue;\n                }\n                throw error;\n            }\n        }\n        \n    } catch (error) {\n        handleStreamError(error);\n        const errorMessage = error instanceof Error ? error.message : String(error);\n        \n        telemetryManager.track('tengu_query_error', {\n            assistantMessages: assistantMessages.length,\n            toolUses: assistantMessages.flatMap(msg => \n                msg.message.content.filter(content => content.type === 'tool_use')\n            ).length\n        });\n        \n        yield* generateFallbackMessages(assistantMessages, errorMessage);\n        yield generateStreamEnd(false, toolUseContext);\n        console.error('Query failed, swallowing error');\n        return;\n    }\n    \n    // Handle abortion (from chunk_0621.js:499-502)\n    if (toolUseContext.abortController.signal.aborted) {\n        yield* generateFallbackMessages(assistantMessages, 'Interrupted by user');\n        yield generateStreamEnd(false, toolUseContext);\n        return;\n    }\n    \n    // No assistant messages - handle edge cases (from chunk_0621.js:503-506)\n    if (!assistantMessages.length) {\n        yield* handleNoAssistantMessages(currentMessages, assistantMessages, systemPrompt, userContext, systemContext, canUseTool, toolUseContext, compactionTracking, fallbackModel, stopHookActive, promptCategory);\n        yield* handleStreamCompletion(currentMessages, assistantMessages, systemPrompt, userContext, systemContext, canUseTool, toolUseContext, compactionTracking, fallbackModel, promptCategory);\n        return;\n    }\n    \n    // Extract tool uses (from chunk_0621.js:507-511)\n    const toolUses = assistantMessages.flatMap(msg => \n        msg.message.content.filter(content => content.type === 'tool_use')\n    );\n    \n    if (!toolUses.length) {\n        yield* handleNoToolUses(currentMessages, assistantMessages, systemPrompt, userContext, systemContext, canUseTool, toolUseContext, compactionTracking, fallbackModel, stopHookActive, promptCategory);\n        return;\n    }\n    \n    // Execute tools (from chunk_0621.js:512-522)\n    const newUserMessages = [];\n    let shouldPreventContinuation = false;\n    \n    for await (const toolResult of executeTools(toolUses, assistantMessages, canUseTool, toolUseContext)) {\n        if (yield toolResult; toolResult && toolResult.type === 'system' && toolResult.preventContinuation) {\n            shouldPreventContinuation = true;\n        }\n        newUserMessages.push(...filterMessagesForType([toolResult], 'user'));\n    }\n    \n    if (toolUseContext.abortController.signal.aborted) {\n        yield generateStreamEnd(true, toolUseContext);\n        return;\n    }\n    \n    if (shouldPreventContinuation) return;\n    \n    // Handle compaction tracking (from chunk_0621.js:523-526)\n    if (compactionTracking?.compacted) {\n        compactionTracking.turnCounter++;\n        telemetryManager.track('tengu_post_autocompact_turn', {\n            turnId: compactionTracking.turnId,\n            turnCounter: compactionTracking.turnCounter\n        });\n    }\n    \n    // Execute queued commands (from chunk_0621.js:527-530)\n    const queuedCommands = [...toolUseContext.getQueuedCommands()];\n    const commandMessages = [];\n    \n    for await (const commandResult of executeQueuedCommands(null, toolUseContext, null, queuedCommands, undefined, messages)) {\n        if (yield commandResult; newUserMessages.push(commandResult); isCommandMessage(commandResult)) {\n            commandMessages.push(commandResult);\n        }\n    }\n    \n    toolUseContext.removeQueuedCommands(queuedCommands);\n    \n    // Handle model fallback for rate limits (from chunk_0621.js:531-542)\n    const rateLimitFallback = checkRateLimitFallback(toolUseContext.options.mainLoopModel);\n    let finalContext = toolUseContext;\n    \n    if (rateLimitFallback) {\n        finalContext = {\n            ...toolUseContext,\n            options: {\n                ...toolUseContext.options,\n                mainLoopModel: rateLimitFallback\n            }\n        };\n        \n        telemetryManager.track('tengu_fallback_system_msg', {\n            mainLoopModel: toolUseContext.options.mainLoopModel,\n            fallbackModel: rateLimitFallback\n        });\n        \n        yield createSystemMessage(`Claude Opus limit reached, now using ${getModelDisplayName(rateLimitFallback)}`, 'warning');\n    }\n    \n    // Recursive call for continuation (from chunk_0621.js:543-558)\n    const continuationContext = {\n        ...finalContext,\n        pendingSteeringAttachments: undefined\n    };\n    \n    yield* streamConversation({\n        messages: [...currentMessages, ...assistantMessages, ...newUserMessages],\n        systemPrompt,\n        userContext,\n        systemContext,\n        canUseTool,\n        toolUseContext: continuationContext,\n        autoCompactTracking: compactionTracking,\n        fallbackModel,\n        stopHookActive,\n        promptCategory\n    });\n}\n\n/**\n * Stream API Request\n * \n * Extracted from chunk_0581.js:2-300+ (KSB function):\n * - Feature flag and off-switch checking\n * - Tool schema preparation and validation\n * - Request formatting and metadata\n * - Streaming response processing\n * - Usage tracking and telemetry\n * \n * @param {Array} messages - Formatted messages\n * @param {Array} systemPrompt - System prompt messages\n * @param {number} maxThinkingTokens - Thinking token limit\n * @param {Array} tools - Available tools\n * @param {AbortSignal} signal - Abort signal\n * @param {Object} options - Request options\n * @returns {AsyncGenerator} Streaming API response\n */\nexport async function* streamAPIRequest(messages, systemPrompt, maxThinkingTokens, tools, signal, options) {\n    // Feature flag checks (from chunk_0581.js:3-8)\n    if (!isWebEnvironment() && (await checkFeatureFlag('tengu-off-switch', { activated: false })).activated && isModelRestricted(options.model)) {\n        telemetryManager.track('tengu_off_switch_query', {});\n        yield generateErrorMessage(new Error('Service temporarily unavailable'), options.model, options.isNonInteractiveSession);\n        return;\n    }\n    \n    // Prepare tools and beta features (from chunk_0581.js:9-12)\n    const [toolSchemas, betaFeatures] = await Promise.all([\n        Promise.all(tools.map(tool => prepareToolSchema(tool, {\n            getToolPermissionContext: options.getToolPermissionContext,\n            tools\n        }))),\n        getBetaFeatures(options.model)\n    ]);\n    \n    // Prepend CLI system prompt if needed (from chunk_0581.js:13)\n    if (options.prependCLISysprompt) {\n        validateSystemPrompt(systemPrompt);\n        systemPrompt = [getCLISystemPrompt(), ...systemPrompt];\n    }\n    \n    const formattedSystemPrompt = formatSystemPrompt(systemPrompt);\n    const shouldUseBeta = isDevelopmentMode() && betaFeatures.length > 0;\n    const temperature = options.temperature ?? getDefaultTemperature();\n    const formattedMessages = formatMessages(messages);\n    \n    // Emit request telemetry (from chunk_0581.js:18-25)\n    telemetryManager.trackRequest({\n        model: options.model,\n        messagesLength: JSON.stringify([\n            ...formattedSystemPrompt,\n            ...formattedMessages,\n            ...toolSchemas,\n            ...(options.extraToolSchemas ?? [])\n        ]).length,\n        temperature,\n        betas: shouldUseBeta ? betaFeatures : [],\n        permissionMode: options.getToolPermissionContext().mode,\n        promptCategory: options.promptCategory\n    });\n    \n    // Initialize timing and state (from chunk_0581.js:26-65)\n    const startTime = Date.now();\n    let firstChunkTime = Date.now();\n    let retryCount = 0;\n    let stream = undefined;\n    let maxTokens = 0;\n    \n    // Request builder function (from chunk_0581.js:30-57)\n    const buildRequest = (overrides) => {\n        const contextSettings = getContextSettings();\n        const maxTokensLimit = overrides.maxTokensOverride \n            ? Math.min(maxThinkingTokens, overrides.maxTokensOverride - 1) \n            : maxThinkingTokens;\n        \n        const additionalParams = getAdditionalParams(\n            getProvider() === 'bedrock' ? getBedrockParams(overrides.model) : []\n        );\n        \n        const thinkingConfig = maxThinkingTokens > 0 ? {\n            budget_tokens: maxTokensLimit,\n            type: 'enabled'\n        } : undefined;\n        \n        const finalMaxTokens = overrides?.maxTokensOverride || \n            options.maxOutputTokensOverride || \n            Math.max(maxThinkingTokens + 1, getDefaultMaxTokens(options.model));\n        \n        return {\n            model: normalizeModelName(options.model),\n            messages: formatMessagesForAPI(formattedMessages),\n            temperature,\n            system: formattedSystemPrompt,\n            tools: [...toolSchemas, ...(options.extraToolSchemas ?? [])],\n            tool_choice: options.toolChoice,\n            ...(shouldUseBeta ? { betas: betaFeatures } : {}),\n            metadata: getRequestMetadata(),\n            max_tokens: finalMaxTokens,\n            thinking: thinkingConfig,\n            ...(contextSettings && shouldUseBeta && betaFeatures.includes('computer-use-2024-10-22') ? {\n                context_management: contextSettings\n            } : {}),\n            ...additionalParams\n        };\n    };\n    \n    // State tracking arrays (from chunk_0581.js:58-65)\n    let responseChunks = [];\n    let firstTokenLatency = 0;\n    let currentMessage = undefined;\n    let contentBlocks = [];\n    let usage = getInitialUsage();\n    let completionId = null;\n    let isFallbackTriggered = false;\n    let finalMaxTokens = 0;\n    \n    try {\n        // Create API stream (from chunk_0581.js:67-82)\n        stream = await withRetry(\n            () => createAPIClient({\n                maxRetries: 0,\n                model: options.model,\n                isNonInteractiveSession: options.isNonInteractiveSession\n            }),\n            async (client, attempt, retryConfig) => {\n                retryCount = attempt;\n                firstChunkTime = Date.now();\n                const request = buildRequest(retryConfig);\n                finalMaxTokens = request.max_tokens;\n                return client.beta.messages.stream(request, { signal });\n            },\n            {\n                showErrors: !options.isNonInteractiveSession,\n                model: options.model,\n                fallbackModel: options.fallbackModel,\n                maxThinkingTokens\n            }\n        );\n        \n        // Reset state for new stream (from chunk_0581.js:82)\n        responseChunks.length = 0;\n        firstTokenLatency = 0;\n        currentMessage = undefined;\n        contentBlocks.length = 0;\n        usage = getInitialUsage();\n        \n        try {\n            let isFirstChunk = true;\n            \n            // Process streaming chunks (from chunk_0581.js:84-200+)\n            for await (const chunk of stream) {\n                if (isFirstChunk) {\n                    console.debug('Stream started - received first chunk');\n                    isFirstChunk = false;\n                }\n                \n                switch (chunk.type) {\n                    case 'message_start':\n                        currentMessage = chunk.message;\n                        firstTokenLatency = Date.now() - firstChunkTime;\n                        usage = combineUsage(usage, chunk.message.usage);\n                        break;\n                        \n                    case 'content_block_start':\n                        switch (chunk.content_block.type) {\n                            case 'tool_use':\n                                contentBlocks[chunk.index] = {\n                                    ...chunk.content_block,\n                                    input: ''\n                                };\n                                break;\n                            case 'server_tool_use':\n                                contentBlocks[chunk.index] = {\n                                    ...chunk.content_block,\n                                    input: ''\n                                };\n                                break;\n                            case 'text':\n                                contentBlocks[chunk.index] = {\n                                    ...chunk.content_block,\n                                    text: ''\n                                };\n                                break;\n                            case 'thinking':\n                                contentBlocks[chunk.index] = {\n                                    ...chunk.content_block,\n                                    thinking: ''\n                                };\n                                break;\n                        }\n                        break;\n                        \n                    case 'content_block_delta':\n                        if (contentBlocks[chunk.index]) {\n                            switch (chunk.delta.type) {\n                                case 'text_delta':\n                                    contentBlocks[chunk.index].text += chunk.delta.text;\n                                    break;\n                                case 'input_json_delta':\n                                    contentBlocks[chunk.index].input += chunk.delta.partial_json;\n                                    break;\n                                case 'thinking_delta':\n                                    contentBlocks[chunk.index].thinking += chunk.delta.content;\n                                    break;\n                            }\n                        }\n                        break;\n                        \n                    case 'content_block_stop':\n                        // Finalize content block\n                        if (contentBlocks[chunk.index]?.type === 'tool_use' && contentBlocks[chunk.index].input) {\n                            try {\n                                contentBlocks[chunk.index].input = JSON.parse(contentBlocks[chunk.index].input);\n                            } catch (error) {\n                                console.warn('Failed to parse tool input JSON:', error);\n                            }\n                        }\n                        break;\n                        \n                    case 'message_delta':\n                        if (chunk.delta.stop_reason) {\n                            currentMessage = {\n                                ...currentMessage,\n                                stop_reason: chunk.delta.stop_reason\n                            };\n                        }\n                        if (chunk.usage) {\n                            usage = combineUsage(usage, chunk.usage);\n                        }\n                        break;\n                        \n                    case 'message_stop':\n                        // Final message assembly\n                        if (currentMessage) {\n                            currentMessage = {\n                                ...currentMessage,\n                                content: contentBlocks.filter(block => block !== undefined)\n                            };\n                        }\n                        break;\n                }\n                \n                // Yield formatted chunk\n                yield formatStreamChunk(chunk, {\n                    currentMessage,\n                    contentBlocks,\n                    usage,\n                    firstTokenLatency,\n                    startTime,\n                    retryCount\n                });\n            }\n            \n        } catch (streamError) {\n            handleStreamingError(streamError, options);\n            throw streamError;\n        }\n        \n    } catch (error) {\n        handleAPIError(error, options);\n        throw error;\n    }\n    \n    // Emit completion telemetry\n    const totalTime = Date.now() - startTime;\n    telemetryManager.trackCompletion({\n        model: options.model,\n        usage,\n        firstTokenLatency,\n        totalTime,\n        retryCount,\n        maxTokens: finalMaxTokens\n    });\n}\n\n// Helper functions (extracted from various chunks)\n\n/**\n * Add authentication headers\n * From chunk_0557.js:146-149\n */\nfunction addAuthenticationHeaders(headers) {\n    const authToken = process.env.ANTHROPIC_AUTH_TOKEN || getStoredAuthToken();\n    if (authToken) {\n        headers.Authorization = `Bearer ${authToken}`;\n    }\n}\n\n/**\n * Get custom headers from environment\n * From chunk_0557.js:151-165\n */\nfunction getCustomHeaders() {\n    const customHeaders = {};\n    const headerString = process.env.ANTHROPIC_CUSTOM_HEADERS;\n    \n    if (!headerString) return customHeaders;\n    \n    const lines = headerString.split(/\\n|\\r\\n/);\n    for (const line of lines) {\n        if (!line.trim()) continue;\n        \n        const match = line.match(/^\\s*(.*?)\\s*:\\s*(.*?)\\s*$/);\n        if (match) {\n            const [, name, value] = match;\n            if (name && value !== undefined) {\n                customHeaders[name] = value;\n            }\n        }\n    }\n    \n    return customHeaders;\n}\n\n// Placeholder functions for missing implementations\nfunction getUserAgent() { return 'Claude-Code/1.0.0'; }\nfunction initializeAuthentication() { return Promise.resolve(); }\nfunction isWebEnvironment() { return false; }\nfunction getFetchOptions() { return {}; }\nfunction shouldUseCustomFetch() { return false; }\nfunction getCustomFetch() { return fetch; }\nfunction getDefaultAWSRegion() { return 'us-east-1'; }\nfunction getAWSCredentials() { return Promise.resolve(null); }\nfunction getVertexRegion() { return 'us-central1'; }\nfunction getAPIKey() { return process.env.ANTHROPIC_API_KEY; }\nfunction getWebAuthToken() { return null; }\nfunction getStoredAuthToken() { return null; }\nfunction compactMessages(messages) { return Promise.resolve({ messages }); }\nfunction autoCompactMessages(messages) { return Promise.resolve({ messages, wasCompacted: false }); }\nfunction generateTurnId() { return Math.random().toString(36); }\nfunction resolveModelForQuery() { return 'claude-3-sonnet-20240229'; }\nfunction formatMessagesForAPI(messages) { return messages; }\nfunction buildSystemPrompt() { return []; }\nfunction streamAPIRequest() { return []; }\nfunction generateFallbackMessages() { return []; }\nfunction createSystemMessage() { return {}; }\nfunction handleStreamError() { /* Implementation needed */ }\nfunction generateStreamEnd() { return {}; }\nfunction handleNoAssistantMessages() { return []; }\nfunction handleStreamCompletion() { return []; }\nfunction handleNoToolUses() { return []; }\nfunction executeTools() { return []; }\nfunction filterMessagesForType() { return []; }\nfunction executeQueuedCommands() { return []; }\nfunction isCommandMessage() { return false; }\nfunction checkRateLimitFallback() { return null; }\nfunction getModelDisplayName() { return ''; }\nfunction checkFeatureFlag() { return Promise.resolve({ activated: false }); }\nfunction isModelRestricted() { return false; }\nfunction generateErrorMessage() { return {}; }\nfunction prepareToolSchema() { return Promise.resolve({}); }\nfunction getBetaFeatures() { return Promise.resolve([]); }\nfunction validateSystemPrompt() { /* Implementation needed */ }\nfunction getCLISystemPrompt() { return {}; }\nfunction formatSystemPrompt() { return []; }\nfunction isDevelopmentMode() { return false; }\nfunction getDefaultTemperature() { return 0.0; }\nfunction formatMessages() { return []; }\nfunction getContextSettings() { return null; }\nfunction getAdditionalParams() { return {}; }\nfunction getBedrockParams() { return []; }\nfunction getProvider() { return 'anthropic'; }\nfunction getRequestMetadata() { return {}; }\nfunction getDefaultMaxTokens() { return 4096; }\nfunction normalizeModelName() { return ''; }\nfunction formatMessagesForAPI() { return []; }\nfunction getInitialUsage() { return {}; }\nfunction withRetry() { return Promise.resolve(); }\nfunction combineUsage() { return {}; }\nfunction formatStreamChunk() { return {}; }\nfunction handleStreamingError() { /* Implementation needed */ }\nfunction handleAPIError() { /* Implementation needed */ }\n\nclass ModelOverloadError extends Error {\n    constructor(message, originalModel, fallbackModel) {\n        super(message);\n        this.originalModel = originalModel;\n        this.fallbackModel = fallbackModel;\n    }\n}